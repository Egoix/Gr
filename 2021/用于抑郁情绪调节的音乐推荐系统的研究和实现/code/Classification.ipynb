{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "data = pd.read_csv('Emotion_features5.csv')\n",
    "feature = data.loc[:, 'tempo':]\n",
    "# print(feature)\n",
    "featureName = list(feature)\n",
    "# print(featureName)\n",
    "color = ['red' if l==1 else 'green' if l==2 else 'blue' if l==3 else 'orange' for l in data['label']]\n",
    "\n",
    "#标准化\n",
    "for name in featureName:\n",
    "    feature[name] = (feature[name]-feature[name].min())/(feature[name].max()-feature[name].min())\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "array = np.array(data)\n",
    "\n",
    "features = feature.values\n",
    "# print(feature)\n",
    "labels = data.loc[:, 'class'].dropna()\n",
    "test_size = 0.20\n",
    "\n",
    "train_d, test_d, train_l, test_l = train_test_split(features, labels, test_size=test_size)\n",
    "\n",
    "#参数寻优\n",
    "model = KNeighborsClassifier()\n",
    "param_grid = [{'n_neighbors':list(range(1,50))}]\n",
    "grid_search = GridSearchCV(model, param_grid, cv=10, scoring='accuracy')\n",
    "grid_search.fit(train_d, train_l)    #使用训练集进行寻优\n",
    "#grid_search.fit(features, labels)     #使用全集进行寻优\n",
    "print(\"the beast params:\",grid_search.best_params_)        #寻找最佳的k\n",
    "# sorted(grid_search.cv_results_.keys())\n",
    "\n",
    "scores = cross_val_score(grid_search, features, labels, cv=13, scoring='accuracy')     #计算十折交叉验证的accuracy\n",
    "f1_micro = cross_val_score(grid_search, features, labels, cv=13, scoring='f1_micro')    #计算十折交叉验证的f1-micro\n",
    "f1_macro = cross_val_score(grid_search, features, labels, cv=13, scoring='f1_macro')    #计算十折交叉验证的f1-macro\n",
    "f1_weighted = cross_val_score(grid_search, features, labels, cv=13, scoring='f1_weighted')    #计算十折交叉验证的recall\n",
    "# print(scores)\n",
    "#计算准确度mean&var\n",
    "print(\"模型的accuracy：\")\n",
    "print(scores)\n",
    "print(np.mean(scores))\n",
    "print(np.var(scores))\n",
    "#计算f1-micro mean&var\n",
    "print(\"模型的f1-micro：\")\n",
    "print(np.mean(f1_micro))\n",
    "print(np.var(f1_micro))\n",
    "#计算f1-macro mean&var\n",
    "print(\"模型的fi-macro：\")\n",
    "print(np.mean(f1_macro))\n",
    "print(np.var(f1_macro))\n",
    "#计算模型的recall mean&var\n",
    "print(\"模型的f1_weighted：\")\n",
    "print(np.mean(f1_weighted))\n",
    "print(np.var(f1_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-primary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN+PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data = pd.read_csv('Emotion_features1.csv')\n",
    "feature = data.loc[:, 'tempo':]\n",
    "# print(feature)\n",
    "featureName = list(feature)\n",
    "# print(featureName)\n",
    "color = ['red' if l==1 else 'green' if l==2 else 'blue' if l==3 else 'orange' for l in data['label']]\n",
    "\n",
    "#标准化\n",
    "for name in featureName:\n",
    "    feature[name] = (feature[name]-feature[name].min())/(feature[name].max()-feature[name].min())\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "array = np.array(data)\n",
    "\n",
    "features = feature.values\n",
    "# print(feature)\n",
    "labels = data.loc[:, 'class'].dropna()\n",
    "test_size = 0.20\n",
    "\n",
    "#参数降维\n",
    "pca = PCA(n_components=5)\n",
    "newfeatures = pca.fit_transform(features)\n",
    "# print(newfeatures)\n",
    "# print(pca.explained_variance_ratio_) #降维后各主成分的贡献率\n",
    "# print(sum(pca.explained_variance_ratio_))\n",
    "\n",
    "\n",
    "train_d, test_d, train_l, test_l = train_test_split(newfeatures, labels, test_size=test_size)\n",
    "\n",
    "# 参数寻优\n",
    "model = KNeighborsClassifier()\n",
    "param_grid = [{'n_neighbors':list(range(1,20))}]\n",
    "grid_search = GridSearchCV(model, param_grid, cv=10, scoring='accuracy')\n",
    "grid_search.fit(train_d, train_l)    #使用训练集进行寻优\n",
    "#grid_search.fit(features, labels)     #使用全集进行寻优\n",
    "print(\"the beast params:\",grid_search.best_params_)        #寻找最佳的k\n",
    "# sorted(grid_search.cv_results_.keys())\n",
    "\n",
    "scores = cross_val_score(grid_search, newfeatures, labels, cv=10, scoring='accuracy')     #计算十折交叉验证的accuracy\n",
    "f1_micro = cross_val_score(grid_search, newfeatures, labels, cv=10, scoring='f1_micro')    #计算十折交叉验证的f1-micro\n",
    "f1_macro = cross_val_score(grid_search, newfeatures, labels, cv=10, scoring='f1_macro')    #计算十折交叉验证的f1-macro\n",
    "f1_weighted = cross_val_score(grid_search, newfeatures, labels, cv=10, scoring='f1_weighted')    #计算十折交叉验证的f1_weighted\n",
    "# print(scores)\n",
    "#计算准确度mean&var\n",
    "print(\"模型的accuracy：\")\n",
    "print(np.mean(scores))\n",
    "print(np.var(scores))\n",
    "#计算f1-micro mean&var\n",
    "print(\"模型的f1-micro：\")\n",
    "print(np.mean(f1_micro))\n",
    "print(np.var(f1_micro))\n",
    "#计算f1-macro mean&var\n",
    "print(\"模型的fi-macro：\")\n",
    "print(np.mean(f1_macro))\n",
    "print(np.var(f1_macro))\n",
    "#计算模型的recall mean&var\n",
    "print(\"模型的f1_weighted：\")\n",
    "print(np.mean(f1_weighted))\n",
    "print(np.var(f1_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN LDA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "data = pd.read_csv('Emotion_features1.csv')\n",
    "feature = data.loc[:, 'tempo':]\n",
    "# print(feature)\n",
    "featureName = list(feature)\n",
    "# print(featureName)\n",
    "color = ['red' if l==1 else 'green' if l==2 else 'blue' if l==3 else 'orange' for l in data['label']]\n",
    "\n",
    "#归一化\n",
    "for name in featureName:\n",
    "    feature[name] = (feature[name]-feature[name].min())/(feature[name].max()-feature[name].min())\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "array = np.array(data)\n",
    "\n",
    "features = feature.values\n",
    "# print(feature)\n",
    "labels = data.loc[:, 'class'].dropna()\n",
    "test_size = 0.20\n",
    "\n",
    "#参数降维\n",
    "lda = LDA(n_components=4)\n",
    "lda.fit(features,labels)\n",
    "newfeatures = lda.transform(features)\n",
    "print(lda.explained_variance_ratio_)    #Percentage of variance explained by each of the selected components. \n",
    "\n",
    "train_d, test_d, train_l, test_l = train_test_split(newfeatures, labels, test_size=test_size)\n",
    "\n",
    "# 参数寻优\n",
    "model = KNeighborsClassifier()\n",
    "param_grid = [{'n_neighbors':list(range(1,50))}]\n",
    "grid_search = GridSearchCV(model, param_grid, cv=10, scoring='accuracy')\n",
    "grid_search.fit(train_d, train_l)    #使用训练集进行寻优\n",
    "#grid_search.fit(features, labels)     #使用全集进行寻优\n",
    "print(\"the beast params:\",grid_search.best_params_)        #寻找最佳的k\n",
    "# sorted(grid_search.cv_results_.keys())\n",
    "\n",
    "scores = cross_val_score(grid_search, newfeatures, labels, cv=10, scoring='accuracy')     #计算十折交叉验证的accuracy\n",
    "f1_micro = cross_val_score(grid_search, newfeatures, labels, cv=10, scoring='f1_micro')    #计算十折交叉验证的f1-micro\n",
    "f1_macro = cross_val_score(grid_search, newfeatures, labels, cv=10, scoring='f1_macro')    #计算十折交叉验证的f1-macro\n",
    "f1_weighted = cross_val_score(grid_search, newfeatures, labels, cv=10, scoring='f1_weighted')    #计算十折交叉验证的f1_weighted\n",
    "# print(scores)\n",
    "#计算准确度mean&var\n",
    "print(\"模型的accuracy：\")\n",
    "print(np.mean(scores))\n",
    "print(np.var(scores))\n",
    "#计算f1-micro mean&var\n",
    "print(\"模型的f1-micro：\")\n",
    "print(np.mean(f1_micro))\n",
    "print(np.var(f1_micro))\n",
    "#计算f1-macro mean&var\n",
    "print(\"模型的fi-macro：\")\n",
    "print(np.mean(f1_macro))\n",
    "print(np.var(f1_macro))\n",
    "#计算模型的recall mean&var\n",
    "print(\"模型的f1_weighted：\")\n",
    "print(np.mean(f1_weighted))\n",
    "print(np.var(f1_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN+PCA+LDA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "data = pd.read_csv('Emotion_features2.csv')\n",
    "feature = data.loc[:, 'tempo':]\n",
    "# print(feature)\n",
    "featureName = list(feature)\n",
    "# print(featureName)\n",
    "color = ['red' if l==1 else 'green' if l==2 else 'blue' if l==3 else 'orange' for l in data['label']]\n",
    "\n",
    "#标准化\n",
    "for name in featureName:\n",
    "    feature[name] = (feature[name]-feature[name].min())/(feature[name].max()-feature[name].min())\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "array = np.array(data)\n",
    "\n",
    "features = feature.values\n",
    "# print(feature)\n",
    "labels = data.loc[:, 'class'].dropna()\n",
    "test_size = 0.20\n",
    "\n",
    "#参数降维\n",
    "pca = PCA(n_components=3)\n",
    "newfeatures = pca.fit_transform(features)\n",
    "# print(newfeatures)\n",
    "# print(pca.explained_variance_ratio_) #降维后各主成分的贡献率\n",
    "# print(sum(pca.explained_variance_ratio_))\n",
    "\n",
    "lda = LDA(n_components=1)\n",
    "lda.fit(newfeatures,labels)\n",
    "ldanewfeatures = lda.transform(newfeatures)\n",
    "\n",
    "\n",
    "train_d, test_d, train_l, test_l = train_test_split(ldanewfeatures, labels, test_size=test_size)\n",
    "\n",
    "# 参数寻优\n",
    "model = KNeighborsClassifier()\n",
    "param_grid = [{'n_neighbors':list(range(1,20))}]\n",
    "grid_search = GridSearchCV(model, param_grid, cv=10, scoring='accuracy')\n",
    "grid_search.fit(train_d, train_l)    #使用训练集进行寻优\n",
    "#grid_search.fit(features, labels)     #使用全集进行寻优\n",
    "print(\"the beast params:\",grid_search.best_params_)        #寻找最佳的k\n",
    "# sorted(grid_search.cv_results_.keys())\n",
    "\n",
    "scores = cross_val_score(model, ldanewfeatures, labels, cv=10, scoring='accuracy')     #计算十折交叉验证的accuracy\n",
    "f1_micro = cross_val_score(model, ldanewfeatures, labels, cv=10, scoring='f1_micro')    #计算十折交叉验证的f1-micro\n",
    "f1_macro = cross_val_score(model, ldanewfeatures, labels, cv=10, scoring='f1_macro')    #计算十折交叉验证的f1-macro\n",
    "f1_weighted = cross_val_score(model, ldanewfeatures, labels, cv=10, scoring='f1_weighted')    #计算十折交叉验证的f1_weighted\n",
    "# print(scores)\n",
    "#计算准确度mean&var\n",
    "print(\"模型的accuracy：\")\n",
    "print(np.mean(scores))\n",
    "print(np.var(scores))\n",
    "#计算f1-micro mean&var\n",
    "print(\"模型的f1-micro：\")\n",
    "print(np.mean(f1_micro))\n",
    "print(np.var(f1_micro))\n",
    "#计算f1-macro mean&var\n",
    "print(\"模型的fi-macro：\")\n",
    "print(np.mean(f1_macro))\n",
    "print(np.var(f1_macro))\n",
    "#计算模型的recall mean&var\n",
    "print(\"模型的f1_weighted：\")\n",
    "print(np.mean(f1_weighted))\n",
    "print(np.var(f1_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-boating",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "data = pd.read_csv('Emotion_features5.csv')\n",
    "feature = data.loc[:, 'tempo':]\n",
    "# print(feature)\n",
    "featureName = list(feature)\n",
    "# print(featureName)\n",
    "color = ['red' if l==1 else 'green' if l==2 else 'blue' if l==3 else 'orange' for l in data['label']]\n",
    "\n",
    "#标准化\n",
    "for name in featureName:\n",
    "    feature[name] = (feature[name]-feature[name].min())/(feature[name].max()-feature[name].min())\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "array = np.array(data)\n",
    "\n",
    "features = feature.values\n",
    "# print(feature)\n",
    "labels = data.loc[:, 'class'].dropna()\n",
    "test_size = 0.20\n",
    "\n",
    "train_d, test_d, train_l, test_l = train_test_split(features, labels, test_size=test_size)\n",
    "\n",
    "#参数寻优\n",
    "parameters = {'kernel':['linear','rbf'], 'C':[0.01, 0.1, 1, 10, 100]}\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, parameters, cv=10)\n",
    "clf.fit(train_d, train_l)\n",
    "print(\"the beast params:\",clf.best_params_)        #寻找最佳的C\n",
    "\n",
    "scores = cross_val_score(clf, features, labels, cv=13, scoring='accuracy')     #计算十折交叉验证的accuracy\n",
    "f1_micro = cross_val_score(clf, features, labels, cv=13, scoring='f1_micro')    #计算十折交叉验证的f1-micro\n",
    "f1_macro = cross_val_score(clf, features, labels, cv=13, scoring='f1_macro')    #计算十折交叉验证的f1-macro\n",
    "f1_weighted = cross_val_score(clf, features, labels, cv=13, scoring='f1_weighted')    #计算十折交叉验证的recall\n",
    "print(scores)\n",
    "#计算准确度mean&var\n",
    "print(\"模型的accuracy：\")\n",
    "print(np.mean(scores))\n",
    "print(np.var(scores))\n",
    "#计算f1-micro mean&var\n",
    "print(\"模型的f1-micro：\")\n",
    "print(np.mean(f1_micro))\n",
    "print(np.var(f1_micro))\n",
    "#计算f1-macro mean&var\n",
    "print(\"模型的fi-macro：\")\n",
    "print(np.mean(f1_macro))\n",
    "print(np.var(f1_macro))\n",
    "#计算模型的recall mean&var\n",
    "print(\"模型的f1_weighted：\")\n",
    "print(np.mean(f1_weighted))\n",
    "print(np.var(f1_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM+PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data = pd.read_csv('Emotion_features1.csv')\n",
    "feature = data.loc[:, 'tempo':]\n",
    "# print(feature)\n",
    "featureName = list(feature)\n",
    "# print(featureName)\n",
    "color = ['red' if l==1 else 'green' if l==2 else 'blue' if l==3 else 'orange' for l in data['label']]\n",
    "\n",
    "#标准化\n",
    "for name in featureName:\n",
    "    feature[name] = (feature[name]-feature[name].min())/(feature[name].max()-feature[name].min())\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "array = np.array(data)\n",
    "\n",
    "features = feature.values\n",
    "labels = data.loc[:, 'class'].dropna()\n",
    "test_size = 0.20\n",
    "\n",
    "#参数降维\n",
    "pca = PCA(n_components=5)\n",
    "newfeatures = pca.fit_transform(features)\n",
    "# print(newfeatures)\n",
    "\n",
    "train_d, test_d, train_l, test_l = train_test_split(newfeatures, labels, test_size=test_size)\n",
    "\n",
    "#参数寻优\n",
    "parameters = {'kernel':['linear'], 'C':[1, 10, 100, 1000, 10000]}\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, parameters, cv=10)\n",
    "clf.fit(train_d, train_l)\n",
    "print(\"the beast params:\",clf.best_params_)        #寻找最佳的C\n",
    "\n",
    "scores = cross_val_score(clf, newfeatures, labels, cv=10, scoring='accuracy')     #计算十折交叉验证的accuracy\n",
    "f1_micro = cross_val_score(clf, newfeatures, labels, cv=10, scoring='f1_micro')    #计算十折交叉验证的f1-micro\n",
    "f1_macro = cross_val_score(clf, newfeatures, labels, cv=10, scoring='f1_macro')    #计算十折交叉验证的f1-macro\n",
    "f1_weighted = cross_val_score(clf, newfeatures, labels, cv=10, scoring='f1_weighted')    #计算十折交叉验证的f1-weighted\n",
    "print(scores)\n",
    "#计算准确度mean&var\n",
    "print(\"模型的accuracy：\")\n",
    "print(np.mean(scores))\n",
    "print(np.var(scores))\n",
    "#计算f1-micro mean&var\n",
    "print(\"模型的f1-micro：\")\n",
    "print(np.mean(f1_micro))\n",
    "print(np.var(f1_micro))\n",
    "#计算f1-macro mean&var\n",
    "print(\"模型的fi-macro：\")\n",
    "print(np.mean(f1_macro))\n",
    "print(np.var(f1_macro))\n",
    "#计算模型的recall mean&var\n",
    "print(\"模型的f1_weighted：\")\n",
    "print(np.mean(f1_weighted))\n",
    "print(np.var(f1_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-drink",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM+LDA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "data = pd.read_csv('Emotion_features1.csv')\n",
    "feature = data.loc[:, 'tempo':]\n",
    "# print(feature)\n",
    "featureName = list(feature)\n",
    "# print(featureName)\n",
    "color = ['red' if l==1 else 'green' if l==2 else 'blue' if l==3 else 'orange' for l in data['label']]\n",
    "\n",
    "#标准化\n",
    "for name in featureName:\n",
    "    feature[name] = (feature[name]-feature[name].min())/(feature[name].max()-feature[name].min())\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "array = np.array(data)\n",
    "\n",
    "features = feature.values\n",
    "labels = data.loc[:, 'class'].dropna()\n",
    "test_size = 0.20\n",
    "\n",
    "#参数降维\n",
    "lda = LDA(n_components=4)\n",
    "lda.fit(features,labels)\n",
    "newfeatures = lda.transform(features)\n",
    "print(lda.explained_variance_ratio_)    #Percentage of variance explained by each of the selected components. \n",
    "\n",
    "train_d, test_d, train_l, test_l = train_test_split(newfeatures, labels, test_size=test_size)\n",
    "\n",
    "#参数寻优\n",
    "parameters = {'kernel':['linear'], 'C':[1, 10, 100, 1000, 10000]}\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, parameters, cv=10)\n",
    "clf.fit(train_d, train_l)\n",
    "print(\"the beast params:\",clf.best_params_)        #寻找最佳的C\n",
    "\n",
    "scores = cross_val_score(clf, newfeatures, labels, cv=10, scoring='accuracy')     #计算十折交叉验证的accuracy\n",
    "f1_micro = cross_val_score(clf, newfeatures, labels, cv=10, scoring='f1_micro')    #计算十折交叉验证的f1-micro\n",
    "f1_macro = cross_val_score(clf, newfeatures, labels, cv=10, scoring='f1_macro')    #计算十折交叉验证的f1-macro\n",
    "f1_weighted = cross_val_score(clf, newfeatures, labels, cv=10, scoring='f1_weighted')    #计算十折交叉验证的f1-weighted\n",
    "print(scores)\n",
    "#计算准确度mean&var\n",
    "print(\"模型的accuracy：\")\n",
    "print(np.mean(scores))\n",
    "print(np.var(scores))\n",
    "#计算f1-micro mean&var\n",
    "print(\"模型的f1-micro：\")\n",
    "print(np.mean(f1_micro))\n",
    "print(np.var(f1_micro))\n",
    "#计算f1-macro mean&var\n",
    "print(\"模型的fi-macro：\")\n",
    "print(np.mean(f1_macro))\n",
    "print(np.var(f1_macro))\n",
    "#计算模型的recall mean&var\n",
    "print(\"模型的f1_weighted：\")\n",
    "print(np.mean(f1_weighted))\n",
    "print(np.var(f1_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-workshop",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM+LDA+PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "data = pd.read_csv('Emotion_features1.csv')\n",
    "feature = data.loc[:, 'tempo':]\n",
    "# print(feature)\n",
    "featureName = list(feature)\n",
    "# print(featureName)\n",
    "color = ['red' if l==1 else 'green' if l==2 else 'blue' if l==3 else 'orange' for l in data['label']]\n",
    "\n",
    "#标准化\n",
    "for name in featureName:\n",
    "    feature[name] = (feature[name]-feature[name].min())/(feature[name].max()-feature[name].min())\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "array = np.array(data)\n",
    "\n",
    "features = feature.values\n",
    "labels = data.loc[:, 'class'].dropna()\n",
    "test_size = 0.20\n",
    "\n",
    "#参数降维\n",
    "pca = PCA(n_components=5)\n",
    "newfeatures = pca.fit_transform(features)\n",
    "\n",
    "lda = LDA(n_components=4)\n",
    "lda.fit(newfeatures,labels)\n",
    "ldanewfeatures = lda.transform(newfeatures)\n",
    "# print(newfeatures)\n",
    "\n",
    "train_d, test_d, train_l, test_l = train_test_split(ldanewfeatures, labels, test_size=test_size)\n",
    "\n",
    "#参数寻优\n",
    "parameters = {'kernel':['linear'], 'C':[1, 10, 100, 1000, 10000]}\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, parameters, cv=10)\n",
    "clf.fit(train_d, train_l)\n",
    "print(\"the beast params:\",clf.best_params_)        #寻找最佳的C\n",
    "\n",
    "scores = cross_val_score(clf, ldanewfeatures, labels, cv=10, scoring='accuracy')     #计算十折交叉验证的accuracy\n",
    "f1_micro = cross_val_score(clf, ldanewfeatures, labels, cv=10, scoring='f1_micro')    #计算十折交叉验证的f1-micro\n",
    "f1_macro = cross_val_score(clf, ldanewfeatures, labels, cv=10, scoring='f1_macro')    #计算十折交叉验证的f1-macro\n",
    "f1_weighted = cross_val_score(clf, ldanewfeatures, labels, cv=10, scoring='f1_weighted')    #计算十折交叉验证的f1-weighted\n",
    "print(scores)\n",
    "#计算准确度mean&var\n",
    "print(\"模型的accuracy：\")\n",
    "print(np.mean(scores))\n",
    "print(np.var(scores))\n",
    "#计算f1-micro mean&var\n",
    "print(\"模型的f1-micro：\")\n",
    "print(np.mean(f1_micro))\n",
    "print(np.var(f1_micro))\n",
    "#计算f1-macro mean&var\n",
    "print(\"模型的fi-macro：\")\n",
    "print(np.mean(f1_macro))\n",
    "print(np.var(f1_macro))\n",
    "#计算模型的recall mean&var\n",
    "print(\"模型的f1_weighted：\")\n",
    "print(np.mean(f1_weighted))\n",
    "print(np.var(f1_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-range",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM+LOO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "\n",
    "data = pd.read_csv('Emotion_features5.csv')\n",
    "feature = data.loc[:, 'average_beats':]\n",
    "# print(feature)\n",
    "featureName = list(feature)\n",
    "# print(featureName)\n",
    "color = ['red' if l==1 else 'green' if l==2 else 'blue' if l==3 else 'orange' for l in data['label']]\n",
    "\n",
    "#标准化\n",
    "for name in featureName:\n",
    "    feature[name] = (feature[name]-feature[name].min())/(feature[name].max()-feature[name].min())\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "array = np.array(data)\n",
    "\n",
    "features = feature.values\n",
    "print(feature)\n",
    "labels = data.loc[:, 'class'].dropna()\n",
    "\n",
    "# pca = PCA(n_components=5)\n",
    "# newfeatures = pca.fit_transform(features)\n",
    "\n",
    "# lda = LDA(n_components=4)\n",
    "# lda.fit(newfeatures,labels)\n",
    "# ldanewfeatures = lda.transform(newfeatures)\n",
    "\n",
    "model = svm.SVC()\n",
    "parameters = {'kernel':['linear'], 'C':[0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]}\n",
    "grid_search = GridSearchCV(model, parameters, cv=10, scoring='accuracy')\n",
    "\n",
    "pre= []\n",
    "aaa = []\n",
    "loo = LeaveOneOut()\n",
    "acc= []\n",
    "macro = []\n",
    "micro = []\n",
    "weighted = []\n",
    "\n",
    "for m in range(5):\n",
    "    for train, test in loo.split(features):\n",
    "        train_X, test_X, train_y, test_y = features[train],features[test],labels[train],labels[test]\n",
    "        grid_search.fit(train_X,train_y)\n",
    "        predicted = grid_search.predict(test_X)\n",
    "        pre.append(predicted)\n",
    "        aaa.append(test_y)\n",
    "    acc.append(accuracy_score(aaa, pre))  \n",
    "    micro.append(f1_score(aaa, pre, average='micro'))\n",
    "    macro.append(f1_score(aaa, pre, average='macro')) \n",
    "    weighted.append(f1_score(aaa, pre, average='weighted'))\n",
    "\n",
    "print(acc)   \n",
    "print(micro)\n",
    "print(macro)\n",
    "print(weighted)\n",
    "print(\"模型的accuracy：\")\n",
    "print(np.mean(acc))\n",
    "print(np.var(acc))\n",
    "#计算f1-micro mean&var\n",
    "print(\"模型的f1-micro：\")\n",
    "print(np.mean(micro))\n",
    "print(np.var(micro))\n",
    "#计算f1-macro mean&var\n",
    "print(\"模型的fi-macro：\")\n",
    "print(np.mean(macro))\n",
    "print(np.var(macro))\n",
    "#计算模型的recall mean&var\n",
    "print(\"模型的f1_weighted：\")\n",
    "print(np.mean(weighted))\n",
    "print(np.var(weighted))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-tablet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep Forest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.decomposition import PCA\n",
    "from deepforest import CascadeForestClassifier\n",
    "\n",
    "data = pd.read_csv('Emotion_features2.csv')\n",
    "feature = data.loc[:, 'tempo':]\n",
    "# print(feature)\n",
    "featureName = list(feature)\n",
    "# print(featureName)\n",
    "color = ['red' if l==1 else 'green' if l==2 else 'blue' if l==3 else 'orange' for l in data['label']]\n",
    "\n",
    "#标准化\n",
    "for name in featureName:\n",
    "    feature[name] = (feature[name]-feature[name].min())/(feature[name].max()-feature[name].min())\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "array = np.array(data)\n",
    "\n",
    "features = feature.values\n",
    "# print(feature)\n",
    "labels = data.loc[:, 'class'].dropna()\n",
    "test_size = 0.20\n",
    "\n",
    "\n",
    "train_d, test_d, train_l, test_l = train_test_split(newfeatures, labels, test_size=test_size)\n",
    "\n",
    "#参数寻优\n",
    "model = CascadeForestClassifier(n_estimators = 1, n_trees = 13, max_depth = 2, random_state=1)\n",
    "#model = CascadeForestClassifier(n_estimators = 1, n_trees = 13, random_state=1)\n",
    "model.fit(train_d, train_l)\n",
    "\n",
    "scores = cross_val_score(model, newfeatures, labels, cv=10, scoring='accuracy')     #计算十折交叉验证的accuracy\n",
    "f1_micro = cross_val_score(model, newfeatures, labels, cv=10, scoring='f1_micro')    #计算十折交叉验证的f1-micro\n",
    "f1_macro = cross_val_score(model, newfeatures, labels, cv=10, scoring='f1_macro')    #计算十折交叉验证的f1-macro\n",
    "f1_weighted = cross_val_score(model, newfeatures, labels, cv=10, scoring='f1_weighted')    #计算十折交叉验证的f1-weighted\n",
    "print(scores)\n",
    "#计算准确度mean&var\n",
    "print(\"模型的accuracy：\")\n",
    "print(np.mean(scores))\n",
    "print(np.var(scores))\n",
    "#计算f1-micro mean&var\n",
    "print(\"模型的f1-micro：\")\n",
    "print(np.mean(f1_micro))\n",
    "print(np.var(f1_micro))\n",
    "#计算f1-macro mean&var\n",
    "print(\"模型的fi-macro：\")\n",
    "print(np.mean(f1_macro))\n",
    "print(np.var(f1_macro))\n",
    "#计算模型的recall mean&var\n",
    "print(\"模型的f1_weighted：\")\n",
    "print(np.mean(f1_weighted))\n",
    "print(np.var(f1_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep Forest+PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.decomposition import PCA\n",
    "from deepforest import CascadeForestClassifier\n",
    "\n",
    "data = pd.read_csv('Emotion_features2.csv')\n",
    "feature = data.loc[:, 'tempo':]\n",
    "# print(feature)\n",
    "featureName = list(feature)\n",
    "# print(featureName)\n",
    "color = ['red' if l==1 else 'green' if l==2 else 'blue' if l==3 else 'orange' for l in data['label']]\n",
    "\n",
    "#标准化\n",
    "for name in featureName:\n",
    "    feature[name] = (feature[name]-feature[name].min())/(feature[name].max()-feature[name].min())\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "array = np.array(data)\n",
    "\n",
    "features = feature.values\n",
    "# print(feature)\n",
    "labels = data.loc[:, 'class'].dropna()\n",
    "test_size = 0.20\n",
    "\n",
    "#参数降维\n",
    "pca = PCA(n_components=3)\n",
    "newfeatures = pca.fit_transform(features)\n",
    "# print(newfeatures)\n",
    "# print(pca.explained_variance_ratio_) #降维后各主成分的贡献率\n",
    "# print(sum(pca.explained_variance_ratio_))\n",
    "\n",
    "\n",
    "train_d, test_d, train_l, test_l = train_test_split(newfeatures, labels, test_size=test_size)\n",
    "\n",
    "#参数寻优\n",
    "model = CascadeForestClassifier(n_estimators = 1, n_trees = 13, max_depth = 2, random_state=1)\n",
    "#model = CascadeForestClassifier(n_estimators = 1, n_trees = 13, random_state=1)\n",
    "model.fit(train_d, train_l)\n",
    "\n",
    "scores = cross_val_score(model, newfeatures, labels, cv=10, scoring='accuracy')     #计算十折交叉验证的accuracy\n",
    "f1_micro = cross_val_score(model, newfeatures, labels, cv=10, scoring='f1_micro')    #计算十折交叉验证的f1-micro\n",
    "f1_macro = cross_val_score(model, newfeatures, labels, cv=10, scoring='f1_macro')    #计算十折交叉验证的f1-macro\n",
    "f1_weighted = cross_val_score(model, newfeatures, labels, cv=10, scoring='f1_weighted')    #计算十折交叉验证的f1-weighted\n",
    "print(scores)\n",
    "#计算准确度mean&var\n",
    "print(\"模型的accuracy：\")\n",
    "print(np.mean(scores))\n",
    "print(np.var(scores))\n",
    "#计算f1-micro mean&var\n",
    "print(\"模型的f1-micro：\")\n",
    "print(np.mean(f1_micro))\n",
    "print(np.var(f1_micro))\n",
    "#计算f1-macro mean&var\n",
    "print(\"模型的fi-macro：\")\n",
    "print(np.mean(f1_macro))\n",
    "print(np.var(f1_macro))\n",
    "#计算模型的recall mean&var\n",
    "print(\"模型的f1_weighted：\")\n",
    "print(np.mean(f1_weighted))\n",
    "print(np.var(f1_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-third",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DeepForest+LDA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.decomposition import PCA\n",
    "from deepforest import CascadeForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "data = pd.read_csv('Emotion_features1.csv')\n",
    "feature = data.loc[:, 'tempo':]\n",
    "# print(feature)\n",
    "featureName = list(feature)\n",
    "# print(featureName)\n",
    "color = ['red' if l==1 else 'green' if l==2 else 'blue' if l==3 else 'orange' for l in data['label']]\n",
    "\n",
    "#标准化\n",
    "for name in featureName:\n",
    "    feature[name] = (feature[name]-feature[name].min())/(feature[name].max()-feature[name].min())\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "array = np.array(data)\n",
    "\n",
    "features = feature.values\n",
    "# print(feature)\n",
    "labels = data.loc[:, 'class'].dropna()\n",
    "test_size = 0.20\n",
    "\n",
    "#参数降维\n",
    "lda = LDA(n_components=1)\n",
    "lda.fit(features,labels)\n",
    "newfeatures = lda.transform(features)\n",
    "print(lda.explained_variance_ratio_)    #Percentage of variance explained by each of the selected components. \n",
    "\n",
    "train_d, test_d, train_l, test_l = train_test_split(newfeatures, labels, test_size=test_size)\n",
    "\n",
    "#参数寻优\n",
    "model = CascadeForestClassifier(random_state=1)\n",
    "#model = CascadeForestClassifier(n_estimators = 1, n_trees = 13, random_state=1)\n",
    "model.fit(train_d, train_l)\n",
    "\n",
    "scores = cross_val_score(model, newfeatures, labels, cv=10, scoring='accuracy')     #计算十折交叉验证的accuracy\n",
    "f1_micro = cross_val_score(model, newfeatures, labels, cv=10, scoring='f1_micro')    #计算十折交叉验证的f1-micro\n",
    "f1_macro = cross_val_score(model, newfeatures, labels, cv=10, scoring='f1_macro')    #计算十折交叉验证的f1-macro\n",
    "f1_weighted = cross_val_score(model, newfeatures, labels, cv=10, scoring='f1_weighted')    #计算十折交叉验证的f1-weighted\n",
    "print(scores)\n",
    "#计算准确度mean&var\n",
    "print(\"模型的accuracy：\")\n",
    "print(np.mean(scores))\n",
    "print(np.var(scores))\n",
    "#计算f1-micro mean&var\n",
    "print(\"模型的f1-micro：\")\n",
    "print(np.mean(f1_micro))\n",
    "print(np.var(f1_micro))\n",
    "#计算f1-macro mean&var\n",
    "print(\"模型的fi-macro：\")\n",
    "print(np.mean(f1_macro))\n",
    "print(np.var(f1_macro))\n",
    "#计算模型的recall mean&var\n",
    "print(\"模型的f1_weighted：\")\n",
    "print(np.mean(f1_weighted))\n",
    "print(np.var(f1_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DeepForest+PCA+LDA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.decomposition import PCA\n",
    "from deepforest import CascadeForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "data = pd.read_csv('Emotion_features2.csv')\n",
    "feature = data.loc[:, 'tempo':]\n",
    "# print(feature)\n",
    "featureName = list(feature)\n",
    "# print(featureName)\n",
    "color = ['red' if l==1 else 'green' if l==2 else 'blue' if l==3 else 'orange' for l in data['label']]\n",
    "\n",
    "#标准化\n",
    "for name in featureName:\n",
    "    feature[name] = (feature[name]-feature[name].min())/(feature[name].max()-feature[name].min())\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "array = np.array(data)\n",
    "\n",
    "features = feature.values\n",
    "# print(feature)\n",
    "labels = data.loc[:, 'class'].dropna()\n",
    "test_size = 0.20\n",
    "\n",
    "#参数降维\n",
    "pca = PCA(n_components=3)\n",
    "newfeatures = pca.fit_transform(features)\n",
    "\n",
    "lda = LDA(n_components=1)\n",
    "lda.fit(newfeatures,labels)\n",
    "ldanewfeatures = lda.transform(newfeatures)\n",
    "print(lda.explained_variance_ratio_)    #Percentage of variance explained by each of the selected components. \n",
    "\n",
    "train_d, test_d, train_l, test_l = train_test_split(ldanewfeatures, labels, test_size=test_size)\n",
    "\n",
    "#参数寻优\n",
    "model = CascadeForestClassifier(random_state=1)\n",
    "#model = CascadeForestClassifier(n_estimators = 1, n_trees = 13, random_state=1)\n",
    "model.fit(train_d, train_l)\n",
    "\n",
    "scores = cross_val_score(model, ldanewfeatures, labels, cv=10, scoring='accuracy')     #计算十折交叉验证的accuracy\n",
    "f1_micro = cross_val_score(model, ldanewfeatures, labels, cv=10, scoring='f1_micro')    #计算十折交叉验证的f1-micro\n",
    "f1_macro = cross_val_score(model, ldanewfeatures, labels, cv=10, scoring='f1_macro')    #计算十折交叉验证的f1-macro\n",
    "f1_weighted = cross_val_score(model, ldanewfeatures, labels, cv=10, scoring='f1_weighted')    #计算十折交叉验证的f1-weighted\n",
    "print(scores)\n",
    "#计算准确度mean&var\n",
    "print(\"模型的accuracy：\")\n",
    "print(np.mean(scores))\n",
    "print(np.var(scores))\n",
    "#计算f1-micro mean&var\n",
    "print(\"模型的f1-micro：\")\n",
    "print(np.mean(f1_micro))\n",
    "print(np.var(f1_micro))\n",
    "#计算f1-macro mean&var\n",
    "print(\"模型的fi-macro：\")\n",
    "print(np.mean(f1_macro))\n",
    "print(np.var(f1_macro))\n",
    "#计算模型的recall mean&var\n",
    "print(\"模型的f1_weighted：\")\n",
    "print(np.mean(f1_weighted))\n",
    "print(np.var(f1_weighted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
